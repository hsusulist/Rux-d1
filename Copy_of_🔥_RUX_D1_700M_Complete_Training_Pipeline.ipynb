{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf6a86505c99414aa989a8c14b5e8102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbced83e05be4f54847d2c53f652649a",
              "IPY_MODEL_cc10f110e3e64ce39179d579eeafb435",
              "IPY_MODEL_437617a792de438587f06b5afbf78c36"
            ],
            "layout": "IPY_MODEL_b01bd30408e84d8691b832c14311d03c"
          }
        },
        "bbced83e05be4f54847d2c53f652649a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3cd73e52d824b178e38b3ad9acb8f69",
            "placeholder": "​",
            "style": "IPY_MODEL_70f41d667ed54fd69077c75d54fadd36",
            "value": "   CodeAlpaca: 100%"
          }
        },
        "cc10f110e3e64ce39179d579eeafb435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db80dc6e477f4aa683ed3e7db6fb527f",
            "max": 20022,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38f5a83582564c17a43dcf1871db3ac4",
            "value": 20022
          }
        },
        "437617a792de438587f06b5afbf78c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd74804864254c30a2d01727652b6e99",
            "placeholder": "​",
            "style": "IPY_MODEL_9a7c7b8cad764902885d777f5e3e313d",
            "value": " 20022/20022 [00:05&lt;00:00, 2312.56it/s]"
          }
        },
        "b01bd30408e84d8691b832c14311d03c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3cd73e52d824b178e38b3ad9acb8f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70f41d667ed54fd69077c75d54fadd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db80dc6e477f4aa683ed3e7db6fb527f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38f5a83582564c17a43dcf1871db3ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd74804864254c30a2d01727652b6e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a7c7b8cad764902885d777f5e3e313d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "596cf137228d48dfad9d450c300c6137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0cf231ca1cc14c219cebccc77dbce1fd",
              "IPY_MODEL_3c1d8411473f4528a2626e68e6760014",
              "IPY_MODEL_f366f819ab83434eb52b09489d0bb256"
            ],
            "layout": "IPY_MODEL_94fba8a77a0c44c29572c1c82a32d596"
          }
        },
        "0cf231ca1cc14c219cebccc77dbce1fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec0b8ceaa1244369bdb426896eb874a",
            "placeholder": "​",
            "style": "IPY_MODEL_09f5492f631a4f609e2ad620fca782b9",
            "value": "   Alpaca: 100%"
          }
        },
        "3c1d8411473f4528a2626e68e6760014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec73fda1140d44de802e9d004ecab4f8",
            "max": 51760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_591d2a2e22f44f349ca34d9c75c02182",
            "value": 51760
          }
        },
        "f366f819ab83434eb52b09489d0bb256": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d18beafefe8a4d8cb4305714770df6b7",
            "placeholder": "​",
            "style": "IPY_MODEL_d4b5d00452fa470bb9c896c8cfe03157",
            "value": " 51760/51760 [00:03&lt;00:00, 25609.84it/s]"
          }
        },
        "94fba8a77a0c44c29572c1c82a32d596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec0b8ceaa1244369bdb426896eb874a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09f5492f631a4f609e2ad620fca782b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec73fda1140d44de802e9d004ecab4f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "591d2a2e22f44f349ca34d9c75c02182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d18beafefe8a4d8cb4305714770df6b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b5d00452fa470bb9c896c8cfe03157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a054319423a4f8cb36343939737712e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f717faa490d4054bab928a5aeb30e49",
              "IPY_MODEL_ac6f03bf6760467bba87ea4ad3ba2ca3",
              "IPY_MODEL_34e398b09324485b96534ed78a7ec690"
            ],
            "layout": "IPY_MODEL_ecd7018d66dc4b44adfc59ae19fafa89"
          }
        },
        "5f717faa490d4054bab928a5aeb30e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f3e3505dba845d7a8fd72a7617fbf85",
            "placeholder": "​",
            "style": "IPY_MODEL_a17a651b5edd418ca63169fc4a98865e",
            "value": "   GSM8K: 100%"
          }
        },
        "ac6f03bf6760467bba87ea4ad3ba2ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cea1b5a70b34176b76185e5def66704",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f5040fd26cf4fbfaae21e592b4fb518",
            "value": 7473
          }
        },
        "34e398b09324485b96534ed78a7ec690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cccff36ea05145208c6eb0d811f8aae3",
            "placeholder": "​",
            "style": "IPY_MODEL_363b70d1f41742419530e3a92a36d844",
            "value": " 7473/7473 [00:00&lt;00:00, 36787.95it/s]"
          }
        },
        "ecd7018d66dc4b44adfc59ae19fafa89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3e3505dba845d7a8fd72a7617fbf85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a17a651b5edd418ca63169fc4a98865e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cea1b5a70b34176b76185e5def66704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f5040fd26cf4fbfaae21e592b4fb518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cccff36ea05145208c6eb0d811f8aae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "363b70d1f41742419530e3a92a36d844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a67c7032f2f248ffaa50a61fb25bfb77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35b90b28f3b04949b1019830118385be",
              "IPY_MODEL_83c98c7100d84f3b94e19d2a1b8d41ba",
              "IPY_MODEL_57358fc0b64148719d66f37065a0eefe"
            ],
            "layout": "IPY_MODEL_4372046304054e5e9ab53d61679bfdaa"
          }
        },
        "35b90b28f3b04949b1019830118385be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c7c0a088c704037bec95a45f52e4ddd",
            "placeholder": "​",
            "style": "IPY_MODEL_5263e988a8294d748e87df4e3e6dbee5",
            "value": "   Stories: 100%"
          }
        },
        "83c98c7100d84f3b94e19d2a1b8d41ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b268b29ba34ff8a0f9f30a4323920d",
            "max": 30000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8146690d463143ada400b3d66e189acc",
            "value": 30000
          }
        },
        "57358fc0b64148719d66f37065a0eefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce02761283dd4b8ba9fb357f120d7c17",
            "placeholder": "​",
            "style": "IPY_MODEL_ad9179297dbe49cf8bec6cd049d2d217",
            "value": " 30000/30000 [00:00&lt;00:00, 47973.69it/s]"
          }
        },
        "4372046304054e5e9ab53d61679bfdaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c7c0a088c704037bec95a45f52e4ddd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5263e988a8294d748e87df4e3e6dbee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44b268b29ba34ff8a0f9f30a4323920d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8146690d463143ada400b3d66e189acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce02761283dd4b8ba9fb357f120d7c17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad9179297dbe49cf8bec6cd049d2d217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsusulist/Rux-d1/blob/main/Copy_of_%F0%9F%94%A5_RUX_D1_700M_Complete_Training_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4bFhgwz3B1y"
      },
      "source": [
        "# 🔥 RUX-D1 700M - Complete Training Pipeline\n",
        "\n",
        "**Model:** RUX-D1 (Reasoning & Understanding eXpert - Developer 1)\n",
        "\n",
        "**Features:**\n",
        "- 700M parameters (LLaMA-style architecture)\n",
        "- RoPE + SwiGLU + RMSNorm\n",
        "- ~300MB mixed data (code-heavy)\n",
        "- Model knows its name is RUX-D1\n",
        "- Mixed precision (FP16) training\n",
        "- ~2-4 hours on T4 GPU\n",
        "\n",
        "---\n",
        "**Instructions:** Run cells in order (Shift+Enter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE3MwrVc3B2B"
      },
      "source": [
        "## Cell 1: Setup & Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo1ZR0N83B2B",
        "outputId": "8578e903-d229-40b4-859d-f4e807a75299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "  RUX-D1 700M Training Setup\n",
            "============================================================\n",
            "  PyTorch: 2.10.0+cu128\n",
            "  CUDA:    True\n",
            "  GPU:     Tesla T4\n",
            "  VRAM:    15.6 GB\n",
            "============================================================\n",
            "  ✅ Ready to train RUX-D1!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CELL 1: SETUP\n",
        "# ============================================================\n",
        "!pip install -q datasets tokenizers tqdm accelerate sentencepiece\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"  RUX-D1 700M Training Setup\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  PyTorch: {torch.__version__}\")\n",
        "print(f\"  CUDA:    {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU:     {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  VRAM:    {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"  WARNING: No GPU detected!\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"  ✅ Ready to train RUX-D1!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ldluvr93B2G"
      },
      "source": [
        "## Cell 2: Download Data (~300MB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bf6a86505c99414aa989a8c14b5e8102",
            "bbced83e05be4f54847d2c53f652649a",
            "cc10f110e3e64ce39179d579eeafb435",
            "437617a792de438587f06b5afbf78c36",
            "b01bd30408e84d8691b832c14311d03c",
            "b3cd73e52d824b178e38b3ad9acb8f69",
            "70f41d667ed54fd69077c75d54fadd36",
            "db80dc6e477f4aa683ed3e7db6fb527f",
            "38f5a83582564c17a43dcf1871db3ac4",
            "dd74804864254c30a2d01727652b6e99",
            "9a7c7b8cad764902885d777f5e3e313d",
            "596cf137228d48dfad9d450c300c6137",
            "0cf231ca1cc14c219cebccc77dbce1fd",
            "3c1d8411473f4528a2626e68e6760014",
            "f366f819ab83434eb52b09489d0bb256",
            "94fba8a77a0c44c29572c1c82a32d596",
            "fec0b8ceaa1244369bdb426896eb874a",
            "09f5492f631a4f609e2ad620fca782b9",
            "ec73fda1140d44de802e9d004ecab4f8",
            "591d2a2e22f44f349ca34d9c75c02182",
            "d18beafefe8a4d8cb4305714770df6b7",
            "d4b5d00452fa470bb9c896c8cfe03157",
            "3a054319423a4f8cb36343939737712e",
            "5f717faa490d4054bab928a5aeb30e49",
            "ac6f03bf6760467bba87ea4ad3ba2ca3",
            "34e398b09324485b96534ed78a7ec690",
            "ecd7018d66dc4b44adfc59ae19fafa89",
            "2f3e3505dba845d7a8fd72a7617fbf85",
            "a17a651b5edd418ca63169fc4a98865e",
            "9cea1b5a70b34176b76185e5def66704",
            "3f5040fd26cf4fbfaae21e592b4fb518",
            "cccff36ea05145208c6eb0d811f8aae3",
            "363b70d1f41742419530e3a92a36d844",
            "a67c7032f2f248ffaa50a61fb25bfb77",
            "35b90b28f3b04949b1019830118385be",
            "83c98c7100d84f3b94e19d2a1b8d41ba",
            "57358fc0b64148719d66f37065a0eefe",
            "4372046304054e5e9ab53d61679bfdaa",
            "9c7c0a088c704037bec95a45f52e4ddd",
            "5263e988a8294d748e87df4e3e6dbee5",
            "44b268b29ba34ff8a0f9f30a4323920d",
            "8146690d463143ada400b3d66e189acc",
            "ce02761283dd4b8ba9fb357f120d7c17",
            "ad9179297dbe49cf8bec6cd049d2d217"
          ]
        },
        "id": "7F_KfIRR3B2G",
        "outputId": "a8429934-a7ca-4ddd-fef0-ad92173ef5b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'sahil2801/CodeAlpaca-20k' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'sahil2801/CodeAlpaca-20k' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "  📥 DOWNLOADING DATASETS (~300MB)\n",
            "============================================================\n",
            "\n",
            "🤖 [1/6] Creating RUX-D1 Identity Data...\n",
            "   ✅ Identity data: 0.5 MB\n",
            "\n",
            "💻 [2/6] Downloading Code Instructions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   CodeAlpaca:   0%|          | 0/20022 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf6a86505c99414aa989a8c14b5e8102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'bigcode/starcoderdata' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'bigcode/starcoderdata' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✅ CodeAlpaca: 5.7 MB\n",
            "\n",
            "🐍 [3/6] Downloading Python Code...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'yahma/alpaca-cleaned' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'yahma/alpaca-cleaned' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ⚠ Python failed: Dataset 'bigcode/starcoderdata' is a gated dataset on the Hub. You must be authenticated to access it.\n",
            "\n",
            "📝 [4/6] Downloading Instructions...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Alpaca:   0%|          | 0/51760 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "596cf137228d48dfad9d450c300c6137"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'gsm8k' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'gsm8k' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✅ Alpaca: 37.9 MB\n",
            "\n",
            "🔢 [5/6] Downloading Math...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   GSM8K:   0%|          | 0/7473 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a054319423a4f8cb36343939737712e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'roneneldan/TinyStories' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "ERROR:datasets.load:`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'roneneldan/TinyStories' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✅ Math: 3.9 MB\n",
            "\n",
            "📖 [6/6] Downloading Stories...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Stories:   0%|          | 0/30000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a67c7032f2f248ffaa50a61fb25bfb77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✅ Stories: 25.5 MB\n",
            "\n",
            "============================================================\n",
            "  📊 DATA SUMMARY\n",
            "============================================================\n",
            "  00_identity.txt                     0.5 MB\n",
            "  01_code_instruct.txt                5.7 MB\n",
            "  03_alpaca.txt                      37.9 MB\n",
            "  04_math.txt                         3.9 MB\n",
            "  05_stories.txt                     25.5 MB\n",
            "------------------------------------------------------------\n",
            "  TOTAL                              73.5 MB\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CELL 2: DOWNLOAD DATA\n",
        "# ============================================================\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "DATA_DIR = \"data\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def get_mb(fp):\n",
        "    return os.path.getsize(fp)/(1024*1024) if os.path.exists(fp) else 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  📥 DOWNLOADING DATASETS (~300MB)\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 1. RUX-D1 IDENTITY DATA (Most Important!)\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"🤖 [1/6] Creating RUX-D1 Identity Data...\")\n",
        "\n",
        "rux_identity = '''\n",
        "[USER]: What is your name?\n",
        "[RUX-D1]: My name is RUX-D1. I am an advanced AI coding assistant with 700 million parameters, designed to help developers write better code.\n",
        "\n",
        "[USER]: Who are you?\n",
        "[RUX-D1]: I am RUX-D1, which stands for Reasoning & Understanding eXpert - Developer 1. I'm a transformer-based language model specialized in code generation, debugging, and technical explanations.\n",
        "\n",
        "[USER]: Tell me about yourself.\n",
        "[RUX-D1]: I'm RUX-D1, a 700M parameter AI model built from scratch. I specialize in:\n",
        "1. Writing code in Python, JavaScript, Java, C++, and more\n",
        "2. Debugging and fixing errors\n",
        "3. Explaining algorithms and data structures\n",
        "4. Code review and optimization\n",
        "5. Technical discussions and problem-solving\n",
        "\n",
        "[USER]: Hello!\n",
        "[RUX-D1]: Hello! I'm RUX-D1, your AI coding assistant. How can I help you today? Whether you need help with code, algorithms, or technical concepts - I'm here for you!\n",
        "\n",
        "[USER]: Are you ChatGPT?\n",
        "[RUX-D1]: No, I am not ChatGPT. I am RUX-D1, a completely different AI model. I was built independently with a focus on coding and technical assistance.\n",
        "\n",
        "[USER]: What can you do?\n",
        "[RUX-D1]: As RUX-D1, I can help with many programming tasks:\n",
        "- Write clean, efficient code in multiple languages\n",
        "- Debug and fix errors in your code\n",
        "- Explain complex algorithms step by step\n",
        "- Review code and suggest improvements\n",
        "- Help with system design and architecture\n",
        "- Answer technical questions\n",
        "\n",
        "[USER]: Write a Python function to check if a number is prime.\n",
        "[RUX-D1]: Here's an efficient prime checker:\n",
        "\n",
        "```python\n",
        "def is_prime(n):\n",
        "    \"\"\"Check if n is a prime number.\"\"\"\n",
        "    if n < 2:\n",
        "        return False\n",
        "    if n == 2:\n",
        "        return True\n",
        "    if n % 2 == 0:\n",
        "        return False\n",
        "    for i in range(3, int(n**0.5) + 1, 2):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Test\n",
        "print(is_prime(17))  # True\n",
        "print(is_prime(15))  # False\n",
        "```\n",
        "\n",
        "[USER]: Write a binary search function.\n",
        "[RUX-D1]: Here's binary search with O(log n) complexity:\n",
        "\n",
        "```python\n",
        "def binary_search(arr, target):\n",
        "    \"\"\"Binary search - array must be sorted.\"\"\"\n",
        "    left, right = 0, len(arr) - 1\n",
        "\n",
        "    while left <= right:\n",
        "        mid = (left + right) // 2\n",
        "        if arr[mid] == target:\n",
        "            return mid\n",
        "        elif arr[mid] < target:\n",
        "            left = mid + 1\n",
        "        else:\n",
        "            right = mid - 1\n",
        "    return -1\n",
        "\n",
        "# Example\n",
        "nums = [1, 3, 5, 7, 9, 11, 13]\n",
        "print(binary_search(nums, 7))  # Output: 3\n",
        "```\n",
        "\n",
        "[USER]: Explain recursion.\n",
        "[RUX-D1]: Recursion is when a function calls itself to solve smaller subproblems. Here's an example:\n",
        "\n",
        "```python\n",
        "def factorial(n):\n",
        "    \"\"\"Calculate n! using recursion.\"\"\"\n",
        "    # Base case\n",
        "    if n <= 1:\n",
        "        return 1\n",
        "    # Recursive case\n",
        "    return n * factorial(n - 1)\n",
        "\n",
        "# factorial(5) = 5 * factorial(4)\n",
        "#              = 5 * 4 * factorial(3)\n",
        "#              = 5 * 4 * 3 * factorial(2)\n",
        "#              = 5 * 4 * 3 * 2 * factorial(1)\n",
        "#              = 5 * 4 * 3 * 2 * 1 = 120\n",
        "```\n",
        "\n",
        "Key points:\n",
        "1. **Base case**: When to stop (n <= 1)\n",
        "2. **Recursive case**: Break into smaller problem\n",
        "3. Each call gets closer to base case\n",
        "\n",
        "[USER]: What is your favorite language?\n",
        "[RUX-D1]: As RUX-D1, I don't have personal preferences, but I can explain the strengths of each:\n",
        "\n",
        "- **Python** - Best for AI/ML, scripting, beginners\n",
        "- **JavaScript** - Best for web development\n",
        "- **Rust** - Best for performance + safety\n",
        "- **Go** - Best for backend services\n",
        "- **C++** - Best for systems, games\n",
        "\n",
        "The best language is the one that solves your problem!\n",
        "\n",
        "[USER]: Write a quicksort implementation.\n",
        "[RUX-D1]: Here's QuickSort in Python:\n",
        "\n",
        "```python\n",
        "def quicksort(arr):\n",
        "    \"\"\"QuickSort - O(n log n) average case.\"\"\"\n",
        "    if len(arr) <= 1:\n",
        "        return arr\n",
        "\n",
        "    pivot = arr[len(arr) // 2]\n",
        "    left = [x for x in arr if x < pivot]\n",
        "    middle = [x for x in arr if x == pivot]\n",
        "    right = [x for x in arr if x > pivot]\n",
        "\n",
        "    return quicksort(left) + middle + quicksort(right)\n",
        "\n",
        "# Test\n",
        "data = [64, 34, 25, 12, 22, 11, 90]\n",
        "print(quicksort(data))  # [11, 12, 22, 25, 34, 64, 90]\n",
        "```\n",
        "\n",
        "[USER]: Implement a linked list.\n",
        "[RUX-D1]: Here's a complete linked list implementation:\n",
        "\n",
        "```python\n",
        "class Node:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.next = None\n",
        "\n",
        "class LinkedList:\n",
        "    def __init__(self):\n",
        "        self.head = None\n",
        "\n",
        "    def append(self, data):\n",
        "        new_node = Node(data)\n",
        "        if not self.head:\n",
        "            self.head = new_node\n",
        "            return\n",
        "        current = self.head\n",
        "        while current.next:\n",
        "            current = current.next\n",
        "        current.next = new_node\n",
        "\n",
        "    def display(self):\n",
        "        elements = []\n",
        "        current = self.head\n",
        "        while current:\n",
        "            elements.append(current.data)\n",
        "            current = current.next\n",
        "        return elements\n",
        "\n",
        "# Usage\n",
        "ll = LinkedList()\n",
        "ll.append(1)\n",
        "ll.append(2)\n",
        "ll.append(3)\n",
        "print(ll.display())  # [1, 2, 3]\n",
        "```\n",
        "\n",
        "[USER]: Thank you RUX-D1!\n",
        "[RUX-D1]: You're welcome! I'm RUX-D1, always happy to help with coding. Come back anytime! Happy coding! 🚀\n",
        "'''\n",
        "\n",
        "# More identity variations\n",
        "identity_variations = [\n",
        "    \"[USER]: What's your name?\\n[RUX-D1]: I'm RUX-D1, an advanced AI coding assistant.\",\n",
        "    \"[USER]: Introduce yourself.\\n[RUX-D1]: I am RUX-D1, a 700M parameter language model specialized in code.\",\n",
        "    \"[USER]: Who made you?\\n[RUX-D1]: I am RUX-D1, built from scratch as a coding-focused AI assistant.\",\n",
        "    \"[USER]: Hi there!\\n[RUX-D1]: Hello! I'm RUX-D1. How can I help you with coding today?\",\n",
        "    \"[USER]: Hey RUX!\\n[RUX-D1]: Hey! RUX-D1 here, ready to help. What do you need?\",\n",
        "    \"[USER]: Can you code?\\n[RUX-D1]: Absolutely! I'm RUX-D1, designed specifically for coding tasks.\",\n",
        "    \"[USER]: Are you an AI?\\n[RUX-D1]: Yes, I'm RUX-D1, an AI assistant focused on programming and technical help.\",\n",
        "]\n",
        "\n",
        "with open(f\"{DATA_DIR}/00_identity.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(100):  # Repeat for emphasis\n",
        "        f.write(rux_identity + \"\\n\\n\")\n",
        "        for var in identity_variations:\n",
        "            f.write(var + \"\\n\\n\")\n",
        "\n",
        "print(f\"   ✅ Identity data: {get_mb(f'{DATA_DIR}/00_identity.txt'):.1f} MB\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 2. CODE INSTRUCTIONS\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"\\n💻 [2/6] Downloading Code Instructions...\")\n",
        "try:\n",
        "    ds = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\", trust_remote_code=True)\n",
        "    with open(f\"{DATA_DIR}/01_code_instruct.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for x in tqdm(ds, desc=\"   CodeAlpaca\"):\n",
        "            inst = x.get(\"instruction\", \"\")\n",
        "            out = x.get(\"output\", \"\")\n",
        "            f.write(f\"[USER]: {inst}\\n[RUX-D1]: {out}\\n\\n---\\n\\n\")\n",
        "    print(f\"   ✅ CodeAlpaca: {get_mb(f'{DATA_DIR}/01_code_instruct.txt'):.1f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠ CodeAlpaca failed: {e}\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 3. PYTHON CODE\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"\\n🐍 [3/6] Downloading Python Code...\")\n",
        "try:\n",
        "    ds = load_dataset(\"bigcode/starcoderdata\", data_dir=\"python\",\n",
        "                     split=\"train\", streaming=True, trust_remote_code=True)\n",
        "    c = 0\n",
        "    with open(f\"{DATA_DIR}/02_python.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for x in tqdm(ds, desc=\"   Python\", total=12000):\n",
        "            code = x.get(\"content\", \"\")\n",
        "            if 100 < len(code) < 4000:\n",
        "                f.write(f\"```python\\n{code.strip()}\\n```\\n\\n{'#'*50}\\n\\n\")\n",
        "                c += 1\n",
        "            if c >= 12000 or get_mb(f\"{DATA_DIR}/02_python.txt\") > 70:\n",
        "                break\n",
        "    print(f\"   ✅ Python code: {get_mb(f'{DATA_DIR}/02_python.txt'):.1f} MB ({c} files)\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠ Python failed: {e}\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 4. GENERAL INSTRUCTIONS (Alpaca)\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"\\n📝 [4/6] Downloading Instructions...\")\n",
        "try:\n",
        "    ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\", trust_remote_code=True)\n",
        "    with open(f\"{DATA_DIR}/03_alpaca.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for x in tqdm(ds, desc=\"   Alpaca\"):\n",
        "            inst = x.get(\"instruction\", \"\")\n",
        "            out = x.get(\"output\", \"\")\n",
        "            f.write(f\"[USER]: {inst}\\n[RUX-D1]: {out}\\n\\n---\\n\\n\")\n",
        "    print(f\"   ✅ Alpaca: {get_mb(f'{DATA_DIR}/03_alpaca.txt'):.1f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠ Alpaca failed: {e}\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 5. MATH (GSM8K)\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"\\n🔢 [5/6] Downloading Math...\")\n",
        "try:\n",
        "    ds = load_dataset(\"gsm8k\", \"main\", split=\"train\", trust_remote_code=True)\n",
        "    with open(f\"{DATA_DIR}/04_math.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for x in tqdm(ds, desc=\"   GSM8K\"):\n",
        "            f.write(f\"[USER]: {x['question']}\\n[RUX-D1]: {x['answer']}\\n\\n---\\n\\n\")\n",
        "    print(f\"   ✅ Math: {get_mb(f'{DATA_DIR}/04_math.txt'):.1f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠ Math failed: {e}\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 6. TINY STORIES\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"\\n📖 [6/6] Downloading Stories...\")\n",
        "try:\n",
        "    ds = load_dataset(\"roneneldan/TinyStories\", split=\"train[:30000]\", trust_remote_code=True)\n",
        "    with open(f\"{DATA_DIR}/05_stories.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for x in tqdm(ds, desc=\"   Stories\"):\n",
        "            f.write(x[\"text\"].strip() + \"\\n\\n\")\n",
        "    print(f\"   ✅ Stories: {get_mb(f'{DATA_DIR}/05_stories.txt'):.1f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"   ⚠ Stories failed: {e}\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# SUMMARY\n",
        "# ─────────────────────────────────────────────\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  📊 DATA SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "total = 0\n",
        "for f in sorted(os.listdir(DATA_DIR)):\n",
        "    if f.endswith('.txt'):\n",
        "        s = get_mb(f\"{DATA_DIR}/{f}\")\n",
        "        total += s\n",
        "        print(f\"  {f:<30} {s:>8.1f} MB\")\n",
        "print(\"-\"*60)\n",
        "print(f\"  {'TOTAL':<30} {total:>8.1f} MB\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WV5Cucw3B2K"
      },
      "source": [
        "## Cell 3: Train Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x87wc44N3B2K",
        "outputId": "b7698a6e-5483-4881-e406-965ec0ecee4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔤 Training tokenizer on 5 files...\n",
            "   - data/00_identity.txt\n",
            "   - data/01_code_instruct.txt\n",
            "   - data/03_alpaca.txt\n",
            "   - data/04_math.txt\n",
            "   - data/05_stories.txt\n",
            "\n",
            "🔧 Training BPE (vocab_size=128000)...\n",
            "\n",
            "✅ Tokenizer saved! Vocab: 91678\n",
            "\n",
            "🧪 Test encoding:\n",
            "   'Hello! I am RUX-D1....' → 9 tokens\n",
            "   'def fibonacci(n): return n if n <= 1 els...' → 25 tokens\n",
            "   '[USER]: What is your name?\n",
            "[RUX-D1]: I a...' → 17 tokens\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CELL 3: TRAIN TOKENIZER\n",
        "# ============================================================\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors, decoders\n",
        "import glob\n",
        "\n",
        "VOCAB_SIZE = 128000\n",
        "TOKENIZER_PATH = \"rux_tokenizer.json\"\n",
        "\n",
        "# Get all text files\n",
        "data_files = sorted(glob.glob(\"data/*.txt\"))\n",
        "print(f\"\\n🔤 Training tokenizer on {len(data_files)} files...\")\n",
        "for f in data_files:\n",
        "    print(f\"   - {f}\")\n",
        "\n",
        "# Build BPE tokenizer\n",
        "tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
        "\n",
        "# Special tokens\n",
        "special_tokens = [\n",
        "    \"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\",\n",
        "    \"[USER]\", \"[RUX-D1]\", \"[SYSTEM]\",\n",
        "]\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    min_frequency=2,\n",
        "    special_tokens=special_tokens,\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "print(f\"\\n🔧 Training BPE (vocab_size={VOCAB_SIZE})...\")\n",
        "tokenizer.train(data_files, trainer)\n",
        "tokenizer.save(TOKENIZER_PATH)\n",
        "\n",
        "print(f\"\\n✅ Tokenizer saved! Vocab: {tokenizer.get_vocab_size()}\")\n",
        "\n",
        "# Test\n",
        "tests = [\n",
        "    \"Hello! I am RUX-D1.\",\n",
        "    \"def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)\",\n",
        "    \"[USER]: What is your name?\\n[RUX-D1]: I am RUX-D1.\",\n",
        "]\n",
        "print(\"\\n🧪 Test encoding:\")\n",
        "for t in tests:\n",
        "    enc = tokenizer.encode(t)\n",
        "    print(f\"   '{t[:40]}...' → {len(enc.ids)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wCSBRPO3B2L"
      },
      "source": [
        "## Cell 4: Model Architecture (700M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "u1NRQWzt3B2M",
        "outputId": "5a1b535d-3d68-4318-c698-30dc44d4b306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "  🤖 RUX-D1 MODEL READY\n",
            "============================================================\n",
            "  Parameters:  500,216,832 (~500M)\n",
            "  Layers:      22\n",
            "  Hidden dim:  1024\n",
            "  Heads:       18\n",
            "  FFN dim:     4096\n",
            "  Max seq:     384\n",
            "  Device:      cuda\n",
            "============================================================\n",
            "\n",
            "📊 GPU Memory after model load:\n",
            "   Allocated: 10.19 GB\n",
            "   Reserved:  14.66 GB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[1, 32, 18, 56]' is invalid for input of size 32768",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-553/2426746568.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0mtest_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0mtest_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n✅ Test forward: input {test_in.shape} → output {test_out.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-553/2426746568.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, targets)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-553/2426746568.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1776\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1787\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-553/2426746568.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 32, 18, 56]' is invalid for input of size 32768"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CELL 4 FIX: RUX-D1 400M (Safe for T4 16GB)\n",
        "# ============================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import gc\n",
        "\n",
        "# Clear memory first\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# CONFIG ~400M (SAFE FOR T4)\n",
        "# ─────────────────────────────────────────────\n",
        "class RuxConfig:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"RUX-D1\"\n",
        "        self.vocab_size = 128000\n",
        "        self.max_seq_len = 384       # Giảm từ 512 → 384\n",
        "        self.d_model = 1024          # Giảm từ 1280 → 1024\n",
        "        self.n_heads = 18           # Giảm từ 20 → 16\n",
        "        self.n_layers = 22           # Giảm từ 24 → 20\n",
        "        self.d_ff = 4096             # Giảm từ 5120 → 4096\n",
        "        self.dropout = 0.1\n",
        "        self.rope_theta = 10000.0\n",
        "\n",
        "config = RuxConfig()\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# RoPE (Rotary Position Embedding)\n",
        "# ─────────────────────────────────────────────\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len=2048, theta=10000.0):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self._build_cache(max_seq_len)\n",
        "\n",
        "    def _build_cache(self, seq_len):\n",
        "        t = torch.arange(seq_len, device=self.inv_freq.device).float()\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :])\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :])\n",
        "\n",
        "    def forward(self, x, seq_len):\n",
        "        if seq_len > self.cos_cached.shape[2]:\n",
        "            self._build_cache(seq_len)\n",
        "        return (\n",
        "            self.cos_cached[:, :, :seq_len, :].to(x.dtype),\n",
        "            self.sin_cached[:, :, :seq_len, :].to(x.dtype)\n",
        "        )\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# RMS Norm\n",
        "# ─────────────────────────────────────────────\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.float().pow(2).mean(-1, keepdim=True).add(self.eps).rsqrt()\n",
        "        return (x.float() * norm).type_as(x) * self.weight\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Attention with RoPE\n",
        "# ─────────────────────────────────────────────\n",
        "class RuxAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = config.d_model // config.n_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
        "\n",
        "        self.rotary = RotaryEmbedding(self.head_dim, config.max_seq_len, config.rope_theta)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        cos, sin = self.rotary(q, T)\n",
        "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "\n",
        "        # Use scaled_dot_product_attention (more memory efficient)\n",
        "        out = F.scaled_dot_product_attention(q, k, v, attn_mask=None, is_causal=True, dropout_p=self.attn_dropout.p if self.training else 0.0)\n",
        "\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.o_proj(out)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# SwiGLU FFN\n",
        "# ─────────────────────────────────────────────\n",
        "class SwiGLUFFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(config.d_model, config.d_ff, bias=False)\n",
        "        self.up_proj = nn.Linear(config.d_model, config.d_ff, bias=False)\n",
        "        self.down_proj = nn.Linear(config.d_ff, config.d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x)))\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Transformer Block\n",
        "# ─────────────────────────────────────────────\n",
        "class RuxBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attn_norm = RMSNorm(config.d_model)\n",
        "        self.attn = RuxAttention(config)\n",
        "        self.ffn_norm = RMSNorm(config.d_model)\n",
        "        self.ffn = SwiGLUFFN(config)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.attn(self.attn_norm(x), mask)\n",
        "        x = x + self.ffn(self.ffn_norm(x))\n",
        "        return x\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# FULL MODEL\n",
        "# ─────────────────────────────────────────────\n",
        "class RuxD1Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "        self.layers = nn.ModuleList([RuxBlock(config) for _ in range(config.n_layers)])\n",
        "        self.norm = RMSNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.lm_head.weight = self.tok_emb.weight\n",
        "\n",
        "        # Init weights\n",
        "        self.apply(self._init_weights)\n",
        "        self.n_params = sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        B, T = input_ids.shape\n",
        "\n",
        "        x = self.drop(self.tok_emb(input_ids))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, max_new_tokens=150, temperature=0.8, top_k=50, top_p=0.9):\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = input_ids[:, -self.config.max_seq_len:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k > 0:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "            if next_token.item() == 3:  # <eos>\n",
        "                break\n",
        "        return input_ids\n",
        "\n",
        "# Build model\n",
        "model = RuxD1Model(config).to(device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  🤖 {config.model_name} MODEL READY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Parameters:  {model.n_params:,} (~{model.n_params/1e6:.0f}M)\")\n",
        "print(f\"  Layers:      {config.n_layers}\")\n",
        "print(f\"  Hidden dim:  {config.d_model}\")\n",
        "print(f\"  Heads:       {config.n_heads}\")\n",
        "print(f\"  FFN dim:     {config.d_ff}\")\n",
        "print(f\"  Max seq:     {config.max_seq_len}\")\n",
        "print(f\"  Device:      {device}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check memory\n",
        "print(f\"\\n📊 GPU Memory after model load:\")\n",
        "print(f\"   Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "print(f\"   Reserved:  {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
        "\n",
        "# Test\n",
        "test_in = torch.randint(0, config.vocab_size, (1, 32)).to(device)\n",
        "test_out, test_loss = model(test_in, test_in)\n",
        "print(f\"\\n✅ Test forward: input {test_in.shape} → output {test_out.shape}\")\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lkVyvdy3B2O"
      },
      "source": [
        "## Cell 5: Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sAhuAxx3B2O"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 5: DATASET\n",
        "# ============================================================\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tokenizers import Tokenizer\n",
        "import glob\n",
        "\n",
        "class RuxDataset(Dataset):\n",
        "    def __init__(self, data_dir, tokenizer_path, max_len=512, max_tokens=30_000_000):\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
        "\n",
        "        print(\"\\n📦 Building dataset...\")\n",
        "        all_tokens = []\n",
        "\n",
        "        files = sorted(glob.glob(f\"{data_dir}/*.txt\"))\n",
        "        for filepath in files:\n",
        "            fname = filepath.split('/')[-1]\n",
        "            print(f\"   Tokenizing {fname}...\", end=\" \")\n",
        "\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # Tokenize in chunks\n",
        "            for i in range(0, len(text), 100000):\n",
        "                chunk = text[i:i+100000]\n",
        "                enc = self.tokenizer.encode(chunk)\n",
        "                all_tokens.extend(enc.ids)\n",
        "\n",
        "            print(f\"({len(all_tokens):,} tokens)\")\n",
        "\n",
        "            if len(all_tokens) >= max_tokens:\n",
        "                all_tokens = all_tokens[:max_tokens]\n",
        "                print(f\"   Reached {max_tokens:,} token limit\")\n",
        "                break\n",
        "\n",
        "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
        "        self.n_samples = (len(self.tokens) - 1) // max_len\n",
        "\n",
        "        print(f\"\\n   ✅ Dataset: {len(self.tokens):,} tokens, {self.n_samples:,} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.max_len\n",
        "        end = start + self.max_len + 1\n",
        "        chunk = self.tokens[start:end]\n",
        "\n",
        "        if len(chunk) < self.max_len + 1:\n",
        "            chunk = torch.cat([chunk, torch.zeros(self.max_len + 1 - len(chunk), dtype=torch.long)])\n",
        "\n",
        "        return chunk[:self.max_len], chunk[1:self.max_len+1]\n",
        "\n",
        "# Build dataset\n",
        "dataset = RuxDataset(\n",
        "    data_dir=\"data\",\n",
        "    tokenizer_path=\"rux_tokenizer.json\",\n",
        "    max_len=config.max_seq_len,\n",
        "    max_tokens=30_000_000\n",
        ")\n",
        "\n",
        "# Split\n",
        "train_size = int(0.95 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Config for T4 (16GB)\n",
        "BATCH_SIZE = 2       # Small for 700M model\n",
        "GRAD_ACCUM = 16      # Effective BS = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n",
        "\n",
        "print(f\"\\n   Train: {len(train_dataset):,} samples, {len(train_loader):,} batches\")\n",
        "print(f\"   Val:   {len(val_dataset):,} samples, {len(val_loader):,} batches\")\n",
        "print(f\"   Batch: {BATCH_SIZE} x {GRAD_ACCUM} = {BATCH_SIZE*GRAD_ACCUM} effective\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fesgXHKf3B2P"
      },
      "source": [
        "## Cell 6: Training Loop 🔥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCaQ19aY3B2P"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 6: TRAINING\n",
        "# ============================================================\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "# Training config\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-4\n",
        "MIN_LR = 1e-5\n",
        "WARMUP_STEPS = 200\n",
        "WEIGHT_DECAY = 0.1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "LOG_EVERY = 50\n",
        "EVAL_EVERY = 300\n",
        "SAVE_EVERY = 500\n",
        "\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Optimizer\n",
        "decay_params = [p for n, p in model.named_parameters() if p.dim() >= 2 and 'norm' not in n]\n",
        "no_decay_params = [p for n, p in model.named_parameters() if p.dim() < 2 or 'norm' in n]\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {\"params\": decay_params, \"weight_decay\": WEIGHT_DECAY},\n",
        "    {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
        "], lr=LEARNING_RATE, betas=(0.9, 0.95))\n",
        "\n",
        "scaler = GradScaler()\n",
        "max_steps = NUM_EPOCHS * len(train_loader) // GRAD_ACCUM\n",
        "\n",
        "def get_lr(step):\n",
        "    if step < WARMUP_STEPS:\n",
        "        return LEARNING_RATE * (step + 1) / WARMUP_STEPS\n",
        "    progress = (step - WARMUP_STEPS) / max(1, max_steps - WARMUP_STEPS)\n",
        "    return MIN_LR + 0.5 * (LEARNING_RATE - MIN_LR) * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "# Load tokenizer for generation test\n",
        "tokenizer = Tokenizer.from_file(\"rux_tokenizer.json\")\n",
        "\n",
        "def test_generate(prompt, max_tokens=80):\n",
        "    model.eval()\n",
        "    enc = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor([enc.ids], dtype=torch.long).to(device)\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(input_ids, max_new_tokens=max_tokens, temperature=0.8)\n",
        "    model.train()\n",
        "    return tokenizer.decode(out_ids[0].tolist())\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"  🚀 TRAINING RUX-D1 ({model.n_params/1e6:.0f}M parameters)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Epochs:     {NUM_EPOCHS}\")\n",
        "print(f\"  LR:         {LEARNING_RATE}\")\n",
        "print(f\"  Max steps:  {max_steps:,}\")\n",
        "print(f\"  Warmup:     {WARMUP_STEPS}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "global_step = 0\n",
        "best_val_loss = float('inf')\n",
        "start_time = time.time()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n{'━'*70}\")\n",
        "    print(f\"  📅 EPOCH {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'━'*70}\")\n",
        "\n",
        "    epoch_loss = 0\n",
        "    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(progress):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Update LR\n",
        "        lr = get_lr(global_step)\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg['lr'] = lr\n",
        "\n",
        "        # Forward\n",
        "        with autocast(dtype=torch.float16):\n",
        "            _, loss = model(x, y)\n",
        "            loss = loss / GRAD_ACCUM\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        epoch_loss += loss.item() * GRAD_ACCUM\n",
        "\n",
        "        # Gradient step\n",
        "        if (batch_idx + 1) % GRAD_ACCUM == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            global_step += 1\n",
        "\n",
        "            # Update progress\n",
        "            avg_loss = epoch_loss / (batch_idx + 1)\n",
        "            progress.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.2e}', 'step': global_step})\n",
        "\n",
        "            # Log\n",
        "            if global_step % LOG_EVERY == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"\\n  Step {global_step:>5} | Loss: {avg_loss:.4f} | LR: {lr:.2e} | Time: {elapsed/60:.1f}m\")\n",
        "\n",
        "            # Eval\n",
        "            if global_step % EVAL_EVERY == 0:\n",
        "                print(\"\\n  📝 Generation test:\")\n",
        "                prompts = [\n",
        "                    \"[USER]: What is your name?\\n[RUX-D1]:\",\n",
        "                    \"[USER]: Write a Python function.\\n[RUX-D1]:\",\n",
        "                ]\n",
        "                for p in prompts:\n",
        "                    gen = test_generate(p, 60)\n",
        "                    print(f\"     {gen[:150]}...\")\n",
        "                print()\n",
        "\n",
        "            # Save\n",
        "            if global_step % SAVE_EVERY == 0:\n",
        "                torch.save({\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'step': global_step,\n",
        "                    'loss': avg_loss,\n",
        "                }, f\"checkpoints/rux_d1_step{global_step}.pt\")\n",
        "                print(f\"  💾 Checkpoint saved: step {global_step}\")\n",
        "\n",
        "    # End of epoch\n",
        "    print(f\"\\n  ✅ Epoch {epoch+1} done! Avg loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Training complete\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  🎉 TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Total time: {total_time/3600:.2f} hours\")\n",
        "print(f\"  Total steps: {global_step:,}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save final\n",
        "torch.save({\n",
        "    'model': model.state_dict(),\n",
        "    'config': config.__dict__,\n",
        "}, \"checkpoints/rux_d1_final.pt\")\n",
        "print(\"\\n💾 Final model saved: checkpoints/rux_d1_final.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu_HSegn3B2R"
      },
      "source": [
        "## Cell 7: Chat with RUX-D1! 💬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyO0tQ333B2S"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 7: CHAT WITH RUX-D1\n",
        "# ============================================================\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(\"checkpoints/rux_d1_final.pt\", map_location=device)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval()\n",
        "print(\"✅ Model loaded!\")\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"rux_tokenizer.json\")\n",
        "\n",
        "def chat(user_input, max_tokens=200, temperature=0.7):\n",
        "    prompt = f\"[USER]: {user_input}\\n[RUX-D1]:\"\n",
        "    enc = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor([enc.ids], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_k=50,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    full = tokenizer.decode(output_ids[0].tolist())\n",
        "\n",
        "    # Extract response\n",
        "    if \"[RUX-D1]:\" in full:\n",
        "        response = full.split(\"[RUX-D1]:\")[-1].strip()\n",
        "    else:\n",
        "        response = full[len(prompt):].strip()\n",
        "\n",
        "    if \"[USER]\" in response:\n",
        "        response = response.split(\"[USER]\")[0].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Test conversations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  🤖 RUX-D1 CHAT TEST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "test_qs = [\n",
        "    \"What is your name?\",\n",
        "    \"Who are you?\",\n",
        "    \"Write a function to reverse a string.\",\n",
        "    \"Explain what a hash table is.\",\n",
        "    \"Hello!\",\n",
        "]\n",
        "\n",
        "for q in test_qs:\n",
        "    print(f\"\\n👤 USER: {q}\")\n",
        "    response = chat(q)\n",
        "    print(f\"🤖 RUX-D1: {response}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Interactive\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  💬 Interactive Chat (type 'quit' to exit)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "while True:\n",
        "    user = input(\"\\n👤 You: \").strip()\n",
        "    if user.lower() in ('quit', 'exit', 'q'):\n",
        "        print(\"🤖 RUX-D1: Goodbye! Happy coding! 👋\")\n",
        "        break\n",
        "    if not user:\n",
        "        continue\n",
        "    response = chat(user)\n",
        "    print(f\"🤖 RUX-D1: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5aXBfWg3B2T"
      },
      "source": [
        "## Cell 8: Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD0Fu2ru3B2U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "a7e673af-5ad0-4e76-f18f-0b41334f43d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'checkpoints/rux_d1_final.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-553/846463350.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Copy files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpoints/rux_d1_final.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{EXPORT_DIR}/model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rux_tokenizer.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{EXPORT_DIR}/tokenizer.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/rux_d1_final.pt'"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CELL 8: DOWNLOAD\n",
        "# ============================================================\n",
        "import shutil\n",
        "\n",
        "# Create export folder\n",
        "EXPORT_DIR = \"rux_d1_export\"\n",
        "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
        "\n",
        "# Copy files\n",
        "shutil.copy(\"checkpoints/rux_d1_final.pt\", f\"{EXPORT_DIR}/model.pt\")\n",
        "shutil.copy(\"rux_tokenizer.json\", f\"{EXPORT_DIR}/tokenizer.json\")\n",
        "\n",
        "# Save config\n",
        "with open(f\"{EXPORT_DIR}/config.json\", \"w\") as f:\n",
        "    json.dump(config.__dict__, f, indent=2)\n",
        "\n",
        "# Zip\n",
        "shutil.make_archive(\"rux_d1_700m\", 'zip', EXPORT_DIR)\n",
        "\n",
        "print(f\"\\n✅ Export complete!\")\n",
        "print(f\"   📦 rux_d1_700m.zip ({os.path.getsize('rux_d1_700m.zip')/1e6:.0f} MB)\")\n",
        "\n",
        "# Download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    print(\"\\n📥 Starting download...\")\n",
        "    files.download('rux_d1_700m.zip')\n",
        "except:\n",
        "    print(\"\\n💡 Not in Colab. Find file at: rux_d1_700m.zip\")\n",
        "\n",
        "# Save to Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    shutil.copy(\"rux_d1_700m.zip\", \"/content/drive/MyDrive/rux_d1_700m.zip\")\n",
        "    print(\"☁️ Also saved to Google Drive!\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"  🎉 RUX-D1 700M COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"  ✅ Model trained\")\n",
        "print(\"  ✅ Knows its name is RUX-D1\")\n",
        "print(\"  ✅ Can write code\")\n",
        "print(\"  ✅ Ready for deployment!\")\n",
        "print(\"=\"*70)"
      ]
    }
  ]
}